{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "3-NeuarlNetwork7.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/JBNU-2021/blob/main/Predictive_Analytics/decision_tree/3_NeuarlNetwork7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFYYjQtRgjJF"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "dKOuRooYgjJK"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.initializers import glorot_uniform\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "Bzji0ta0gjJN"
      },
      "source": [
        "#!pip show tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDlA2nLGgjJO"
      },
      "source": [
        "# Background\n",
        "\n",
        "_Credit default_ can defined as the failure to repay a debt including interest or principal on a loan or security on the due date.This can cause losses for lenders so that preventive measures is a must, in which early detection for potential default can be one of those. This case study can be categorized as the binary classification.\n",
        "\n",
        "Artifical Neural Network (ANN) is one of models for classification problems, having the ability to capture the linier and also the non-linear model trends from data so that it can give predictions for the new data (having the same distributions).\n",
        "\n",
        "In jupyter notebook, the effectiveness of ANN model will be tried to classify the _credit default customer_ and hope that it can reach 95% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp0DIzlCgjJP"
      },
      "source": [
        "# Data Understanding\n",
        "\n",
        "The data used in this task is a public dataset from UCI Machine Learning entitled \"Default of Credit Card Clients Dataset\" containing information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. \n",
        "\n",
        "This dataset contains 30,000 data observations with 25 variables consisting of 1 ID, 23 predictor variables, and 1 response variable as the default payment next month.\n",
        "\n",
        "Here are some samples of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "vYSRYA0rgjJQ"
      },
      "source": [
        "df = pd.read_csv('credit_cards_dataset.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svKikEJigjJS"
      },
      "source": [
        "The description of each column/variable can be seen below :\n",
        "- ID: ID of each client\n",
        "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
        "- SEX: Gender (1=male, 2=female)\n",
        "- EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
        "- MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
        "- AGE: Age in years\n",
        "- PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
        "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
        "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
        "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
        "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
        "- BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
        "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
        "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
        "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
        "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
        "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
        "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
        "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
        "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
        "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
        "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
        "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
        "- default.payment.next.month: Default payment (1=yes, 0=no)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJAlleOUgjJT"
      },
      "source": [
        "## Data Exploratory\n",
        "As we can see the description of each column/variable, those are the numerical data so that the data summary are all based on basic statistics in mean, median, minimum and maximum etc which detailed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sBcjqeegjJU"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGWvRV5ygjJV"
      },
      "source": [
        "Next, we want see the correlation between all of features and label in the dataset by using the Pearson correlation formula below. <br>\n",
        "$$Covarian (S_{xy}) =\\frac{\\sum(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1}$$\n",
        "\n",
        "\n",
        "The plot below is the correlation between all features (predictor variables) toward label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ew2-1NSgjJW"
      },
      "source": [
        "# Using Pearson Correlation\n",
        "plt.figure(figsize=(14,14))\n",
        "cor = df.iloc[:,1:].corr()\n",
        "x = cor [['default.payment.next.month']]\n",
        "sns.heatmap(x, annot=True, cmap=plt.cm.Reds)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K25e9X-mgjJX"
      },
      "source": [
        "As we can see in the plot above, the repayment status of customers (PAY_0 - PAY_6) have the higher correlation towards the label (default.payment.next.month) in compared to other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4V_xFFogjJY"
      },
      "source": [
        "# Data Preparation\n",
        "## Data Cleansing\n",
        "Before implementing the ANN to predict the \"credit default customer\", we have to check the data, whether it needs cleaning or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OqSRIMUgjJY"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX-HMV0PgjJZ"
      },
      "source": [
        "After checking the summary of missing value in the dataset, the result shows that the data has no missing values so that the data is ready to the next stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxs823SegjJa"
      },
      "source": [
        "## Splitting Data to Training and Test Data\n",
        "In this stage, the clean data will be splitted into 2 categories, train data and test data. The train data will be utilized in the training ANN model, and the data test will be used to test the trained model whether the model has good generalization or not in predicting the future data. In this stage, 80% data will be used as the train data and the rest as the test data.\n",
        "\n",
        "Before splitting, the dataset will be grouped into 2 variables, the data from 2nd to 24rd column as the predictor features (the first columns is not included as predictor) will be groped as X, and the data from 25th columns (label)  will be renamed as y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdaLRLeQgjJb"
      },
      "source": [
        "X = df.iloc[:, 1:24].values\n",
        "y = df.iloc[:, 24].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFzdK-NBgjJb"
      },
      "source": [
        "## Data Standardization\n",
        "\n",
        "After splitting data, the numeric data will be standardized by scaling the data to have mean of 0 and variance of 1. \n",
        "$$X_{stand} = \\frac{X - \\mu}{\\sigma}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Y5hpXIgjJc"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQfkG7uHgjJc"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "On the Modeling phase, we create the ANN model with 5 hidden layer (with 50,40,30,20, and 10 neurons respectively) with _relu_ activation function, and 1 output layer with 1 neuron with _sigmoid_ activation function. Furthermore, we choose the 'Adam' optimizer to optimize the parameter in the created model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGh42w7bgjJd"
      },
      "source": [
        "hl   = 5                    # number of hidden layer\n",
        "nohl = [ 50, 40, 30, 20, 10]     # number of neurons in each hidden layer\n",
        "\n",
        "classifier = Sequential()\n",
        "\n",
        "# Hidden Layer\n",
        "for i in range(hl):\n",
        "    if i==0:\n",
        "        classifier.add(Dense(units=nohl[i], input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))\n",
        "    else :\n",
        "        classifier.add(Dense(units=nohl[i], kernel_initializer=glorot_uniform(seed=0), activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "classifier.add(Dense(units=1, kernel_initializer=glorot_uniform(seed=0), activation='sigmoid'))\n",
        "\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1QXWiO9gjJe"
      },
      "source": [
        "Here below the summary of created model architecture by ANN with the parameters needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YFEuZhegjJf"
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3fgWtOHgjJf"
      },
      "source": [
        "After create the model architecture by ANN, we train the model by a certain number of epoch and batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlSd8_DbgjJg"
      },
      "source": [
        "classifier.fit(X_train, y_train, epochs=150, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imoRXoEygjJh"
      },
      "source": [
        "# Evaluation\n",
        "In this classification problem, we evaluate model by looking at how many of their predictions are correct in which the threshold is 50%. This can be plotted into Confusion Matrix.\n",
        "\n",
        "Here is the confusion matrix from the ANN model after doing prediction to the dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNeMcObrgjJh"
      },
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "conf_matr = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "TP = conf_matr[0,0]; FP = conf_matr[0,1]; TN = conf_matr[1,1]; FN = conf_matr[1,0]\n",
        "print('Confusion Matrix : ')\n",
        "print(conf_matr)\n",
        "print()\n",
        "print('True Positive (TP)  : ',TP)\n",
        "print('False Positive (FP) : ',FP)\n",
        "print('True Negative (TN)  : ',TN)\n",
        "print('False Negative (FN) : ',FN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0E0fQzzgjJi"
      },
      "source": [
        "in which \n",
        "- True Positive (TP) means the model predict customer will pay the credit and the prediction is correct.\n",
        "- False Positive (FP) means the model predict customer will will pay the credit and the prediction is incorrect.\n",
        "- True Negative (TN) means the model predict customer will not will pay the credit and the prediction is correct.\n",
        "- False Negative (FN) means the model predict customer will not will pay the credit and the prediction is incorrect.\n",
        "\n",
        "Based of the result above, then we can start doing evaluation using 3 different metrics: accuracy, recall, and precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4AH8QA4gjJj"
      },
      "source": [
        "### Accuracy\n",
        "Accuracy means how many prediction is true compared to the total data. The metric will be calculated by following formula.\n",
        "\n",
        "$$Accuray = \\frac{TP+TN}{TP+TN+FP+FN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiUGHInrgjJj"
      },
      "source": [
        "acc = (TP+TN)/(TP+TN+FP+FN)\n",
        "print('By this metric, only '+ str(round(acc*100)) + '% of them are correctly predicted.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz6B9xW1gjJk"
      },
      "source": [
        "### Precision\n",
        "In this metric (precision), it only concern on how many positive prediction that are actually correct and this will be calculated by formula below.\n",
        " \n",
        "$$Precision = \\frac{TP}{TP+FP}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBzc6LGsgjJk"
      },
      "source": [
        "pre = TP/(TP+FP)\n",
        "print('From those classification result, by calculating the precision, there are '+ str(round(pre*100)) + '% of them who are actually pay the credit.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUDH5sO8gjJl"
      },
      "source": [
        "After reviewing the model performance by the accurary and precision metric, it seems that the created model included the hyper-parameter used  has not be able the reach the 95% accuray so that there are some possible actions which can be taken, such as :\n",
        "- tuning the hyper-parameter tp get the better performance before releasing the model into the real use\n",
        "- release the model while also developing a better model\n",
        "- trying another classfication model (such as decision-tree, Naive-bayes)"
      ]
    }
  ]
}