{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "12-0xgb.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/JBNU-2021/blob/main/Predictive_Analytics/decision_tree/12_0xgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qltYjBh7lvSc"
      },
      "source": [
        "### XGBoosting\n",
        "\n",
        "https://nbviewer.jupyter.org/github/jphall663/interpretable_machine_learning_with_python/blob/master/xgboost_pdp_ice.ipynb?flush_cache=trueXGBoosting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KNys9rplvSe"
      },
      "source": [
        "import numpy as np                   # array, vector, matrix calculations\n",
        "import pandas as pd                  # DataFrame handling\n",
        "#import shapely as shap                          # for consistent, signed variable importance measurements\n",
        "import xgboost as xgb                # gradient boosting machines (GBMs)\n",
        "\n",
        "import matplotlib.pyplot as plt      # plotting\n",
        "pd.options.display.max_columns = 999 # enable display of all columns in notebook\n",
        "\n",
        "# enables display of plots in notebook\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(42)               # set random seed for reproducibility"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WObWx2_5lvSg"
      },
      "source": [
        "#### Import data and clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbIGLyyQlvSh"
      },
      "source": [
        "# import XLS file\n",
        "path = \"./credit_cards_dataset.csv\"\n",
        "data = pd.read_csv(path) # skip the first row of the spreadsheet"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKnCmKpQlvSi"
      },
      "source": [
        "# remove spaces from target column name \n",
        "data = data.rename(columns={'default payment next month': 'DEFAULT_NEXT_MONTH'})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9XYz17wlvSj"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVUT0SdulvSl"
      },
      "source": [
        "'''\n",
        "import pandas_profiling\n",
        "data.profile_report(style={'full_width':True})\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBU8kAGIlvSm"
      },
      "source": [
        "#profile.to_file(outputfile=\"my.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNo30sqylvSn"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haorH-xHlvSo",
        "outputId": "d1cca6e3-8714-4d0c-a07b-ab2ac0fd706b"
      },
      "source": [
        "# assign target and inputs for GBM\n",
        "#y = 'DEFAULT_NEXT_MONTH'\n",
        "y='default.payment.next.month'\n",
        "X = [name for name in data.columns if name not in [y, 'ID', 'Y_Value']]\n",
        "print('y =', y)\n",
        "print('X =', X)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = default.payment.next.month\n",
            "X = ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR3JTanflvSp"
      },
      "source": [
        "data[X].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrlGZgV2lvSq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QQt7c1_lvSq"
      },
      "source": [
        "data[X + [y]].describe() # display descriptive statistics for all columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSbUR4F6lvSr"
      },
      "source": [
        "# displays last column of Pearson correlation matrix as Pandas DataFrame\n",
        "pd.DataFrame(data[X + [y]].corr()[y]).iloc[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16sn0Q3ylvSs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eQgxe-elvSs"
      },
      "source": [
        "# creates a tuple in which positive correlation values are assigned a 1\n",
        "# and negative correlation values are assigned a -1\n",
        "mono_constraints = tuple([int(i) for i in np.sign(data[X + [y]].corr()[y].values[:-1])])\n",
        "\n",
        "# (-1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1OkVpGolvSt"
      },
      "source": [
        "np.random.seed(42) # set random seed for reproducibility\n",
        "split_ratio = 0.7     # 70%/30% train/test split\n",
        "\n",
        "# execute split\n",
        "split = np.random.rand(len(data)) < split_ratio\n",
        "train = data[split]\n",
        "test = data[~split]\n",
        "\n",
        "# summarize split\n",
        "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
        "print('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lBD0LV6lvSu"
      },
      "source": [
        "# XGBoost uses SVMLight data structure, not Numpy arrays or Pandas DataFrames \n",
        "dtrain = xgb.DMatrix(train[X], train[y])\n",
        "dtest = xgb.DMatrix(test[X], test[y])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e6pIcdalvSu"
      },
      "source": [
        "# used to calibrate predictions to mean of y\n",
        "base_y = train[y].mean()\n",
        "\n",
        "# tuning parameters\n",
        "params = {\n",
        "    'objective': 'binary:logistic',             # produces 0-1 probabilities for binary classification\n",
        "    'booster': 'gbtree',                        # base learner will be decision tree\n",
        "    'eval_metric': 'auc',                       # stop training based on maximum AUC, AUC always between 0-1\n",
        "    'eta': 0.08,                                # learning rate\n",
        "    'subsample': 0.9,                           # use 90% of rows in each decision tree\n",
        "    'colsample_bytree': 0.9,                    # use 90% of columns in each decision tree\n",
        "    'max_depth': 15,                            # allow decision trees to grow to depth of 15\n",
        "    'monotone_constraints':mono_constraints,    # 1 = increasing relationship, -1 = decreasing relationship\n",
        "    'base_score': base_y,                       # calibrate predictions to mean of y \n",
        "    'seed': 42                               # set random seed for reproducibility\n",
        "}\n",
        "\n",
        "# watchlist is used for early stopping\n",
        "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "\n",
        "# train model\n",
        "xgb_model = xgb.train(params,                   # set tuning parameters from above                   \n",
        "                      dtrain,                   # training data\n",
        "                      1000,                     # maximum of 1000 iterations (trees)\n",
        "                      evals=watchlist,          # use watchlist for early stopping \n",
        "                      early_stopping_rounds=50, # stop after 50 iterations (trees) without increase in AUC\n",
        "                      verbose_eval=True)        # display iteration progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BUzwm_WlvSw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_O73eEqlvSw"
      },
      "source": [
        "# dtest is DMatrix\n",
        "# shap_values is Numpy array\n",
        "shap_values = xgb_model.predict(dtest, pred_contribs=True, ntree_limit=xgb_model.best_ntree_limit)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgpfe4ZllvSx"
      },
      "source": [
        "ypred=xgb_model.predict(dtest, ntree_limit=xgb_model.best_ntree_limit)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N87oGKXGlvSy"
      },
      "source": [
        "ypred.reshape(-1,1)\n",
        "predictions = np.array([round(value) for value in ypred])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMYvxDkOlvSy"
      },
      "source": [
        "predictions.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJHGB6RXlvSz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "accuracy = accuracy_score(test[y], predictions)\n",
        "cm = confusion_matrix(test[y], predictions)\n",
        "precision = precision_score(test[y], predictions)\n",
        "recall = recall_score(test[y], predictions)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wECP2pVulvS0",
        "outputId": "d8ec321a-b96e-4953-c249-68342c8b053d"
      },
      "source": [
        "print(accuracy)\n",
        "print(cm)\n",
        "print(precision)\n",
        "print(recall)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8172332547963649\n",
            "[[6583  322]\n",
            " [1307  701]]\n",
            "0.6852394916911045\n",
            "0.34910358565737054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTUQakqzlvS0"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "    \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4U1YuSjlvS1"
      },
      "source": [
        "plt.figure()\n",
        "plot_confusion_matrix(cm, classes=['Non_Default','Default'], normalize=False,\n",
        "                      title='Non Normalized confusion matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXA6j7SQm8HC",
        "outputId": "daed76d0-7bb4-47e1-a5ce-2c668ad2d87e"
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 356 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.41.1)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.2.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.0.1)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491648 sha256=f96dde7032e549cf6a720c7a445ff2109dc0cc062e252981b88f17454ef079fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.39.0 slicer-0.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGLBRVPllvS2"
      },
      "source": [
        "# plot Shapley variable importance summary\n",
        "import shap\n",
        "shap.summary_plot(shap_values[:, :-1], test[xgb_model.feature_names])\n",
        "shap.TreeExplainer(shap_values[:, :-1], test[xgb_model.feature_names])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znb7HkmZlvS3"
      },
      "source": [
        "#https://www.kaggle.com/dansbecker/shap-values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxJFQRAwlvS3"
      },
      "source": [
        "### PDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10aObbQRlvS4"
      },
      "source": [
        "def par_dep(xs, frame, model, resolution=20, bins=None):\n",
        "    \n",
        "    \"\"\" Creates Pandas DataFrame containing partial dependence for a \n",
        "        single variable.\n",
        "    \n",
        "    Args:\n",
        "        xs: Variable for which to calculate partial dependence.\n",
        "        frame: Pandas DataFrame for which to calculate partial dependence.\n",
        "        model: XGBoost model for which to calculate partial dependence.\n",
        "        resolution: The number of points across the domain of xs for which \n",
        "                    to calculate partial dependence, default 20.\n",
        "        bins: List of values at which to set xs, default 20 equally-spaced \n",
        "              points between column minimum and maximum.\n",
        "    \n",
        "    Returns:\n",
        "        Pandas DataFrame containing partial dependence values.\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    # turn off pesky Pandas copy warning\n",
        "    pd.options.mode.chained_assignment = None\n",
        "    \n",
        "    # initialize empty Pandas DataFrame with correct column names\n",
        "    par_dep_frame = pd.DataFrame(columns=[xs, 'partial_dependence'])\n",
        "    \n",
        "    # cache original column values \n",
        "    col_cache = frame.loc[:, xs].copy(deep=True)\n",
        "  \n",
        "    # determine values at which to calculate partial dependence\n",
        "    if bins == None:\n",
        "        min_ = frame[xs].min()\n",
        "        max_ = frame[xs].max()\n",
        "        by = (max_ - min_)/resolution\n",
        "        bins = np.arange(min_, max_, by)\n",
        "        \n",
        "    # calculate partial dependence  \n",
        "    # by setting column of interest to constant \n",
        "    # and scoring the altered data and taking the mean of the predictions\n",
        "    for j in bins:\n",
        "        frame.loc[:, xs] = j\n",
        "        dframe = xgb.DMatrix(frame)\n",
        "        par_dep_i = pd.DataFrame(model.predict(dframe, ntree_limit=model.best_ntree_limit))\n",
        "        par_dep_j = par_dep_i.mean()[0]\n",
        "        par_dep_frame = par_dep_frame.append({xs:j,\n",
        "                                              'partial_dependence': par_dep_j}, \n",
        "                                              ignore_index=True)\n",
        "        \n",
        "    # return input frame to original cached state    \n",
        "    frame.loc[:, xs] = col_cache\n",
        "\n",
        "    return par_dep_frame"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUKgdUMRlvS4"
      },
      "source": [
        "par_dep_PAY_0 = par_dep('PAY_0', test[X], xgb_model)         # calculate partial dependence for PAY_0\n",
        "par_dep_LIMIT_BAL = par_dep('LIMIT_BAL', test[X], xgb_model) # calculate partial dependence for LIMIT_BAL\n",
        "par_dep_BILL_AMT1 = par_dep('BILL_AMT1', test[X], xgb_model) # calculate partial dependence for BILL_AMT1\n",
        "\n",
        "# display partial dependence for LIMIT_BAL\n",
        "par_dep_LIMIT_BAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGcECE-DlvS5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlS4ZHxllvS5"
      },
      "source": [
        "def get_percentile_dict(yhat, id_, frame):\n",
        "\n",
        "    \"\"\" Returns the percentiles of a column, yhat, as the indices based on \n",
        "        another column id_.\n",
        "    \n",
        "    Args:\n",
        "        yhat: Column in which to find percentiles.\n",
        "        id_: Id column that stores indices for percentiles of yhat.\n",
        "        frame: Pandas DataFrame containing yhat and id_. \n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of percentile values and index column values.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # create a copy of frame and sort it by yhat\n",
        "    sort_df = frame.copy(deep=True)\n",
        "    sort_df.sort_values(yhat, inplace=True)\n",
        "    sort_df.reset_index(inplace=True)\n",
        "    \n",
        "    # find top and bottom percentiles\n",
        "    percentiles_dict = {}\n",
        "    percentiles_dict[0] = sort_df.loc[0, id_]\n",
        "    percentiles_dict[99] = sort_df.loc[sort_df.shape[0]-1, id_]\n",
        "\n",
        "    # find 10th-90th percentiles\n",
        "    inc = sort_df.shape[0]//10\n",
        "    for i in range(1, 10):\n",
        "        percentiles_dict[i * 10] = sort_df.loc[i * inc,  id_]\n",
        "\n",
        "    return percentiles_dict"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtlgkdZElvS6"
      },
      "source": [
        "# merge GBM predictions onto test data\n",
        "yhat_test = pd.concat([test.reset_index(drop=True), pd.DataFrame(xgb_model.predict(dtest))], axis=1)\n",
        "yhat_test = yhat_test.rename(columns={0:'p_DEFAULT_NEXT_MONTH'})\n",
        "\n",
        "# find percentiles of predictions\n",
        "percentile_dict = get_percentile_dict('p_DEFAULT_NEXT_MONTH', 'ID', yhat_test)\n",
        "\n",
        "# display percentiles dictionary\n",
        "# ID values for rows\n",
        "# from lowest prediction \n",
        "# to highest prediction\n",
        "percentile_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vE9cZ3YlvS6"
      },
      "source": [
        "### ICE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CsA3nZqlvS7"
      },
      "source": [
        "# retreive bins from original partial dependence calculation\n",
        "\n",
        "bins_PAY_0 = list(par_dep_PAY_0['PAY_0'])\n",
        "bins_LIMIT_BAL = list(par_dep_LIMIT_BAL['LIMIT_BAL'])\n",
        "bins_BILL_AMT1 = list(par_dep_BILL_AMT1['BILL_AMT1'])\n",
        "\n",
        "# for each percentile in percentile_dict\n",
        "# create a new column in the par_dep frame \n",
        "# representing the ICE curve for that percentile\n",
        "# and the variables of interest\n",
        "for i in sorted(percentile_dict.keys()):\n",
        "    \n",
        "    col_name = 'Percentile_' + str(i)\n",
        "    \n",
        "    # ICE curves for PAY_0 across percentiles at bins_PAY_0 intervals\n",
        "    par_dep_PAY_0[col_name] = par_dep('PAY_0', \n",
        "                                    test[test['ID'] == int(percentile_dict[i])][X],  \n",
        "                                    xgb_model, \n",
        "                                    bins=bins_PAY_0)['partial_dependence']\n",
        "    \n",
        "    # ICE curves for LIMIT_BAL across percentiles at bins_LIMIT_BAL intervals\n",
        "    par_dep_LIMIT_BAL[col_name] = par_dep('LIMIT_BAL', \n",
        "                                          test[test['ID'] == int(percentile_dict[i])][X], \n",
        "                                          xgb_model, \n",
        "                                          bins=bins_LIMIT_BAL)['partial_dependence']\n",
        "    \n",
        "\n",
        "\n",
        "    # ICE curves for BILL_AMT1 across percentiles at bins_BILL_AMT1 intervals\n",
        "    par_dep_BILL_AMT1[col_name] = par_dep('BILL_AMT1', \n",
        "                                          test[test['ID'] == int(percentile_dict[i])][X],  \n",
        "                                          xgb_model, \n",
        "                                          bins=bins_BILL_AMT1)['partial_dependence']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR5YlG7GlvS8"
      },
      "source": [
        "par_dep_LIMIT_BAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG4d4WVjlvS8"
      },
      "source": [
        "### Plotting partial dependence and ICE to validate and explain monotonic behavior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRQH7pJplvS8"
      },
      "source": [
        "#### Function to plot partial dependence and ICE\n",
        "\n",
        "def plot_par_dep_ICE(xs, par_dep_frame):\n",
        "\n",
        "    \n",
        "    \"\"\" Plots ICE overlayed onto partial dependence for a single variable.\n",
        "    \n",
        "    Args: \n",
        "        xs: Name of variable for which to plot ICE and partial dependence.\n",
        "        par_dep_frame: Name of Pandas DataFrame containing ICE and partial\n",
        "                       dependence values.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize figure and axis\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    # plot ICE curves\n",
        "    par_dep_frame.drop('partial_dependence', axis=1).plot(x=xs, \n",
        "                                                          colormap='gnuplot',\n",
        "                                                          ax=ax)\n",
        "\n",
        "    # overlay partial dependence, annotate plot\n",
        "    par_dep_frame.plot(title='Partial Dependence and ICE for ' + str(xs),\n",
        "                       x=xs, \n",
        "                       y='partial_dependence',\n",
        "                       style='r-', \n",
        "                       linewidth=3, \n",
        "                       ax=ax)\n",
        "\n",
        "    # add legend\n",
        "    _ = plt.legend(bbox_to_anchor=(1.05, 0),\n",
        "                   loc=3, \n",
        "                   borderaxespad=0.)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra_S8Fd0lvS9"
      },
      "source": [
        "plot_par_dep_ICE('LIMIT_BAL', par_dep_LIMIT_BAL) # plot partial dependence and ICE for LIMIT_BAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9iQBSTplvS-"
      },
      "source": [
        "_ = train['LIMIT_BAL'].plot(kind='hist', bins=20, title='Histogram: LIMIT_BAL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUc27a4NlvS-"
      },
      "source": [
        "plot_par_dep_ICE('PAY_0', par_dep_PAY_0) # plot partial dependence and ICE for PAY_0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMXvCRkBlvS-"
      },
      "source": [
        "_ = train['PAY_0'].plot(kind='hist', bins=20, title='Histogram: PAY_0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pn-94pdlvS_"
      },
      "source": [
        "plot_par_dep_ICE('BILL_AMT1', par_dep_BILL_AMT1) # plot partial dependence and ICE for BILL_AMT1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB_QbByUlvS_"
      },
      "source": [
        "_ = train['BILL_AMT1'].plot(kind='hist', bins=20, title='Histogram: BILL_AMT1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTckLIeWlvTA"
      },
      "source": [
        "### Generate reason codes using the Shapley method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pz-FZYVlvTA"
      },
      "source": [
        "test.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRtAnWvzlvTA"
      },
      "source": [
        "decile = 99\n",
        "row = test[test['ID'] == percentile_dict[decile]]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOOhQUoKlvTB"
      },
      "source": [
        "# reset test data index to find riskiest customer in shap_values \n",
        "# sort to find largest positive contributions\n",
        "s_df = pd.DataFrame(shap_values[row.index[0], :][:-1].reshape(23, 1), columns=['Reason Codes'], index=X)\n",
        "s_df.sort_values(by='Reason Codes', inplace=True, ascending=False)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_-3cv7BlvTB"
      },
      "source": [
        "s_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBmK58XzlvTC"
      },
      "source": [
        "_ = s_df[:5].plot(kind='bar', \n",
        "                  title='Top Five Reason Codes for a Risky Customer\\n', \n",
        "                  legend=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "ANcYf5KSlvTC",
        "outputId": "28b50e21-ac8f-4a7d-dfd5-0ee3ac0a9b07"
      },
      "source": [
        "row # helps understand reason codes"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <th>SEX</th>\n",
              "      <th>EDUCATION</th>\n",
              "      <th>MARRIAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>PAY_0</th>\n",
              "      <th>PAY_2</th>\n",
              "      <th>PAY_3</th>\n",
              "      <th>PAY_4</th>\n",
              "      <th>PAY_5</th>\n",
              "      <th>PAY_6</th>\n",
              "      <th>BILL_AMT1</th>\n",
              "      <th>BILL_AMT2</th>\n",
              "      <th>BILL_AMT3</th>\n",
              "      <th>BILL_AMT4</th>\n",
              "      <th>BILL_AMT5</th>\n",
              "      <th>BILL_AMT6</th>\n",
              "      <th>PAY_AMT1</th>\n",
              "      <th>PAY_AMT2</th>\n",
              "      <th>PAY_AMT3</th>\n",
              "      <th>PAY_AMT4</th>\n",
              "      <th>PAY_AMT5</th>\n",
              "      <th>PAY_AMT6</th>\n",
              "      <th>default.payment.next.month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4173</th>\n",
              "      <td>14245</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
              "4173  14245    10000.0    1          2         1   50      2      2      7   \n",
              "\n",
              "      PAY_4  PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  \\\n",
              "4173      7      7      7     2400.0     2400.0     2400.0     2400.0   \n",
              "\n",
              "      BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  \\\n",
              "4173     2400.0     2400.0       0.0       0.0       0.0       0.0       0.0   \n",
              "\n",
              "      PAY_AMT6  default.payment.next.month  \n",
              "4173       0.0                           1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELBrTUB5lvTC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}