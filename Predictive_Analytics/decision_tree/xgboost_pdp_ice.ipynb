{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "xgboost_pdp_ice.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/JBNU-2021/blob/main/Predictive_Analytics/decision_tree/xgboost_pdp_ice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF_Z4KRii4jy"
      },
      "source": [
        "## License \n",
        "\n",
        "Copyright 2017 - 2020 Patrick Hall and the H2O.ai team\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sppVg-qli4j1"
      },
      "source": [
        "**DISCLAIMER:** This notebook is not legal compliance advice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OkbF5Wni4j3"
      },
      "source": [
        "# Engineering Transparency into Your Machine Learning Model with Python and XGBoost\n",
        "#### Monotonic XGBoost models, partial dependence, ICE, and Shapley explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyT7K2Hgi4j4"
      },
      "source": [
        "A key to building interpretable models is to limit their complexity. The more complex a model is, the harder it is to explain and understand. Overly complex models can also make unstable predictions on new data, which is both difficult to explain and makes models harder to trust. Monotonicity constraints not only simplify models, but do so in a way that is somewhat natural for human reasoning, increasing the transparency of predictive models. Under monotonicity constraints, model predictions can only increase or only decrease as an input variable value increases, and the direction of the constraint is typically specified by the user for logical reasons. For instance, a model might be constrained to produce only increasing probabilities of a certain medical condition as a patient's age increases, or to make only increasing predictions for home prices as a home's square footage increases. \n",
        "\n",
        "In this notebook a gradient boosting machine (GBM) is trained with monotonicity constraints to predict credit card payment defaults, using the UCI credit card default data, Python, NumPy, Pandas, and XGBoost. First, the credit card default data is loaded and prepared. Then Pearson correlation with the prediction target is used to determine the direction of the monotonicity constraints for each input variable and the model is trained. After the model is trained, partial dependence and individual conditional expectation (ICE) plots are used to analyze and verify the model's monotonic behavior. Finally an example of creating regulator mandated reason codes from high fidelity Shapley explanations for any model prediction is presented. This combination of monotonic XGBoost, partial dependence, ICE, and Shapley explanations is probably the most direct way to create an interpretable machine learning model today.\n",
        "\n",
        "**Note**: As of the h2o 3.24 \"Yates\" release, Shapley values are supported in h2o, in addition to GBM monotonicity constraints and partial dependence. To see Shapley values and monotonicity constraints for an h2o GBM in action please see: https://github.com/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E6oR-SZi4j7"
      },
      "source": [
        "#### Python imports "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsXrlpvXi4j8"
      },
      "source": [
        "Let's start with Python package imports. NumPy is used for basic arrray, vector, and matrix calculations. Pandas is used for data frame manipulation and plotting, and XGBoost is used to train a GBM with monotonicity constraints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYfC15tyj5G6"
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mE30VYPi4j9"
      },
      "source": [
        "import numpy as np                   # array, vector, matrix calculations\n",
        "import pandas as pd                  # DataFrame handling\n",
        "import shap                          # for consistent, signed variable importance measurements\n",
        "import xgboost as xgb                # gradient boosting machines (GBMs)\n",
        "\n",
        "import matplotlib.pyplot as plt      # plotting\n",
        "pd.options.display.max_columns = 999 # enable display of all columns in notebook\n",
        "\n",
        "# enables display of plots in notebook\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(12345)                # set random seed for reproducibility"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K4nGpH4i4kA"
      },
      "source": [
        "## 1. Download, explore, and prepare UCI credit card default data\n",
        "\n",
        "UCI credit card default data: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
        "\n",
        "The UCI credit card default data contains demographic and payment information about credit card customers in Taiwan in the year 2005. The data set contains 23 input variables: \n",
        "\n",
        "* **`LIMIT_BAL`**: Amount of given credit (NT dollar)\n",
        "* **`SEX`**: 1 = male; 2 = female\n",
        "* **`EDUCATION`**: 1 = graduate school; 2 = university; 3 = high school; 4 = others \n",
        "* **`MARRIAGE`**: 1 = married; 2 = single; 3 = others\n",
        "* **`AGE`**: Age in years \n",
        "* **`PAY_0`, `PAY_2` - `PAY_6`**: History of past payment; `PAY_0` = the repayment status in September, 2005; `PAY_2` = the repayment status in August, 2005; ...; `PAY_6` = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; ...; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
        "* **`BILL_AMT1` - `BILL_AMT6`**: Amount of bill statement (NT dollar). `BILL_AMNT1` = amount of bill statement in September, 2005; `BILL_AMT2` = amount of bill statement in August, 2005; ...; `BILL_AMT6` = amount of bill statement in April, 2005. \n",
        "* **`PAY_AMT1` - `PAY_AMT6`**: Amount of previous payment (NT dollar). `PAY_AMT1` = amount paid in September, 2005; `PAY_AMT2` = amount paid in August, 2005; ...; `PAY_AMT6` = amount paid in April, 2005. \n",
        "\n",
        "These 23 input variables are used to predict the target variable, whether or not a customer defaulted on their credit card bill in late 2005. Because XGBoost accepts only numeric inputs, all variables will be treated as numeric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wCk_x45i4kE"
      },
      "source": [
        "#### Import data and clean\n",
        "The credit card default data is available as an `.xls` file. Pandas reads `.xls` files automatically, so it's used to load the credit card default data and give the prediction target a shorter name: `DEFAULT_NEXT_MONTH`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FiSz7-ti4kF"
      },
      "source": [
        "# import XLS file\n",
        "path = './credit_cards_dataset.csv'\n",
        "data = pd.read_csv(path) # skip the first row of the spreadsheet\n",
        "\n",
        "# remove spaces from target column name \n",
        "#data = data.rename(columns={'default payment next month': 'DEFAULT_NEXT_MONTH'}) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjQ-_aOji4kH"
      },
      "source": [
        "#### Assign modeling roles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPYGLNs_i4kI"
      },
      "source": [
        "The shorthand name `y` is assigned to the prediction target. `X` is assigned to all other input variables in the credit card default data except the row indentifier, `ID`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDRrPD-ci4kJ",
        "outputId": "33252b5b-9a06-4abf-a18d-339dd0dc1dbc"
      },
      "source": [
        "# assign target and inputs for GBM\n",
        "y = 'default.payment.next.month'\n",
        "X = [name for name in data.columns if name not in [y, 'ID']]\n",
        "print('y =', y)\n",
        "print('X =', X)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = default.payment.next.month\n",
            "X = ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0di-KBJi4kM"
      },
      "source": [
        "#### Display descriptive statistics\n",
        "The Pandas `describe()` function displays a brief description of the credit card default data. The input variables `SEX`, `EDUCATION`, `MARRIAGE`, `PAY_0`-`PAY_6`, and the prediction target `DEFAULT_NEXT_MONTH`, are really categorical variables, but they have already been encoded into meaningful numeric, integer values, which is great for XGBoost. Also, there are no missing values in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGT8URJgi4kN"
      },
      "source": [
        "data[X + [y]].describe() # display descriptive statistics for all columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhKW5NdAi4kP"
      },
      "source": [
        "## 2. Investigate pair-wise Pearson correlations for DEFAULT_NEXT_MONTH\n",
        "\n",
        "Monotonic relationships are much easier to explain to colleagues, bosses, customers, and regulators than more complex, non-monotonic relationships and monotonic relationships may also prevent overfitting and excess error due to variance for new data.\n",
        "\n",
        "To train a transparent monotonic classifier, contraints must be supplied to XGBoost that determine whether the learned relationship between an input variable and the prediction target `DEFAULT_NEXT_MONTH` will be increasing for increases in an input variable or decreasing for increases in an input variable. Pearson correlation provides a linear measure of the direction of the relationship between each input variable and the target. If the pair-wise Pearson correlation between an input and `DEFAULT_NEXT_MONTH` is positive, it will be constrained to have an increasing relationship with the predictions for `DEFAULT_NEXT_MONTH`. If the pair-wise Pearson correlation is negative, the input will be constrained to have a decreasing relationship with the predictions for `DEFAULT_NEXT_MONTH`. \n",
        "\n",
        "Constrainsts are supplied to XGBoost in the form of a Python tuple with length equal to the number of inputs. Each item in the tuple is associated with an input variable based on its index in the tuple. The first constraint in the tuple is associated with the first variable in the training data, the second constraint in the tuple is associated with the second variable in the training data, and so on. The constraints themselves take the form of a 1 for a positive relationship and a -1 for a negative relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd3Bmp5Mi4kQ"
      },
      "source": [
        "#### Calculate Pearson correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNBUKlufi4kR"
      },
      "source": [
        "The Pandas `.corr()` function returns the pair-wise Pearson correlation between variables in a Pandas DataFrame. Because `DEFAULT_NEXT_MONTH` is the last column in the `data` DataFrame, the last column of the Pearson correlation matrix indicates the direction of the linear relationship between each input variable and the prediction target, `DEFAULT_NEXT_MONTH`. According to the calculated values, as a customer's balance limit (`LIMIT_BAL`), bill amounts (`BILL_AMT1`-`BILL_AMT6`), and payment amounts (`PAY_AMT1`-`PAY_AMT6`) increase, their probability of default tends to decrease. However as a customer's number of late payments increase (`PAY_0`, `PAY_2`-`PAY6`), their probability of default usually increases. In general, the Pearson correlation values make sense, and they will be used to ensure that the modeled relationships will make sense as well. (Pearson correlation values between the target variable, DEFAULT_NEXT_MONTH, and each input variable are displayed directly below.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptLYBpGUi4kS"
      },
      "source": [
        "# displays last column of Pearson correlation matrix as Pandas DataFrame\n",
        "pd.DataFrame(data[X + [y]].corr()[y]).iloc[:-1] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi-y4Cw8i4kU"
      },
      "source": [
        "#### Create tuple of monotonicity constraints from Pearson correlation values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2m9rr-pi4kU"
      },
      "source": [
        "The last column of the Pearson correlation matrix is transformed from a numeric column in a Pandas DataFrame into a Python tuple of `1`s and `-1`s that will be used to specifiy monotonicity constraints for each input variable in XGBoost. If the Pearson correlation between an input variable and `DEFAULT_NEXT_MONTH` is positive, a positive montonic relationship constraint is specified for that variable using `1`. If the correlation is negative, a negative monotonic constraint is specified using `-1`. (Specifying `0` indicates that no constraints should be used.) The resulting tuple will be passed to XGBoost when the GBM model is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXBq09ZVi4kV"
      },
      "source": [
        "# creates a tuple in which positive correlation values are assigned a 1\n",
        "# and negative correlation values are assigned a -1\n",
        "mono_constraints = tuple([int(i) for i in np.sign(data[X + [y]].corr()[y].values[:-1])])\n",
        "\n",
        "# (-1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JofPi229i4kW"
      },
      "source": [
        "## 3. Train XGBoost with monotonicity constraints\n",
        "\n",
        "XGBoost is a very accurate, open source GBM library for regression and classification tasks. XGBoost can learn complex relationships between input variables and a target variable, but here the `monotone_constraints` tuning parameter is used to enforce monotonicity between inputs and the prediction for `DEFAULT_NEXT_MONTH`. XGBoost's early stopping functionality is also used to limit overfitting to the training data\n",
        "\n",
        "XGBoost is available from: https://github.com/dmlc/xgboost and the implementation of XGBoost is described in detail here: http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf.\n",
        "\n",
        "After training, GBM variable importance is calculated and displayed. GBM variable importance is a global measure of the overall impact of an input variable on the GBM model predictions. Global variable importance values give an indication of the magnitude of a variable's contribution to model predictions for all observations. To enhance trust in the GBM model, variable importance values should typically conform to human domain knowledge and reasonable expectations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxU260YHi4kX"
      },
      "source": [
        "#### Split data into training and test sets for early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd8EK9ysi4kY"
      },
      "source": [
        "The credit card default data is split into training and test sets to monitor and prevent overtraining. Reproducibility is another important factor in creating trustworthy models, and randomly splitting datasets can introduce randomness in model predictions and other results. A random seed is used here to ensure the data split is reproducible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARUP-rONi4kZ",
        "outputId": "25c23b2d-1076-453a-a686-d892b05913f4"
      },
      "source": [
        "np.random.seed(12345) # set random seed for reproducibility\n",
        "split_ratio = 0.7     # 70%/30% train/test split\n",
        "\n",
        "# execute split\n",
        "split = np.random.rand(len(data)) < split_ratio\n",
        "train = data[split]\n",
        "test = data[~split]\n",
        "\n",
        "# summarize split\n",
        "print('Train data rows = %d, columns = %d' % (train.shape[0], train.shape[1]))\n",
        "print('Test data rows = %d, columns = %d' % (test.shape[0], test.shape[1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data rows = 20946, columns = 25\n",
            "Test data rows = 9054, columns = 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogh8VzjAi4ka"
      },
      "source": [
        "#### Train XGBoost GBM classifier\n",
        "To train an XGBoost classifier, the training and test data must be converted from Pandas DataFrames into SVMLight format. The `DMatrix()` function in the XGBoost package is used to convert the data. Many XGBoost tuning parameters must be specified as well. Typically a grid search would be performed to identify the best parameters for a given modeling task. For brevity's sake, a previously-discovered set of good tuning parameters are specified here. Notice that the monotonicity constraints are passed to XGBoost using the `monotone_constraints` parameter. Because gradient boosting methods typically resample training data, an additional random seed is also specified for XGBoost using the `seed` paramter to create reproducible predictions, error rates, and variable importance values. To avoid overfitting, the `early_stopping_rounds` parameter is used to stop the training process after the test area under the curve (AUC) statistic fails to increase for 50 iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KcoR4CHi4ka"
      },
      "source": [
        "# XGBoost uses SVMLight data structure, not Numpy arrays or Pandas DataFrames \n",
        "dtrain = xgb.DMatrix(train[X], train[y])\n",
        "dtest = xgb.DMatrix(test[X], test[y])\n",
        "\n",
        "# used to calibrate predictions to mean of y \n",
        "base_y = train[y].mean()\n",
        "\n",
        "# tuning parameters\n",
        "params = {\n",
        "    'objective': 'binary:logistic',             # produces 0-1 probabilities for binary classification\n",
        "    'booster': 'gbtree',                        # base learner will be decision tree\n",
        "    'eval_metric': 'auc',                       # stop training based on maximum AUC, AUC always between 0-1\n",
        "    'eta': 0.08,                                # learning rate\n",
        "    'subsample': 0.9,                           # use 90% of rows in each decision tree\n",
        "    'colsample_bytree': 0.9,                    # use 90% of columns in each decision tree\n",
        "    'max_depth': 15,                            # allow decision trees to grow to depth of 15\n",
        "    'monotone_constraints': 1,                  # 1 = increasing relationship, -1 = decreasing relationship\n",
        "    'base_score': base_y,                       # calibrate predictions to mean of y \n",
        "    'seed': 12345                               # set random seed for reproducibility\n",
        "}\n",
        "\n",
        "# watchlist is used for early stopping\n",
        "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "\n",
        "# train model\n",
        "xgb_model = xgb.train(params,                   # set tuning parameters from above                   \n",
        "                      dtrain,                   # training data\n",
        "                      1000,                     # maximum of 1000 iterations (trees)\n",
        "                      evals=watchlist,          # use watchlist for early stopping \n",
        "                      early_stopping_rounds=50, # stop after 50 iterations (trees) without increase in AUC\n",
        "                      verbose_eval=True)        # display iteration progress\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcN3220yi4kc"
      },
      "source": [
        "#### Global Shapley variable importance\n",
        "By setting `pred_contribs=True`, XGBoost's `predict()` function will return Shapley values for each row of the test set. Instead of relying on traditional single-value variable importance measures, local Shapley values for each input will be ploted below to get a more holistic and consisent measurement for the global importance of each input variable. Shapley values are introduced in greater detail in Section 6 below, but for now notice the monotonicity of the input variable contributions displayed in the Shapley summary plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEMtwE2ui4kc"
      },
      "source": [
        "# dtest is DMatrix\n",
        "# shap_values is Numpy array\n",
        "shap_values = xgb_model.predict(dtest, pred_contribs=True, ntree_limit=xgb_model.best_ntree_limit)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaQ064BHi4kd"
      },
      "source": [
        "# plot Shapley variable importance summary \n",
        "shap.summary_plot(shap_values[:, :-1], test[xgb_model.feature_names])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vT2sv5oi4ke"
      },
      "source": [
        "####  Display Shapley variable importance summary\n",
        "The variable importance ranking should be parsimonious with human domain knowledge and reasonable expectations. In this case, `PAY_0` is by far the most important variable. As someone's most recent behavior is a very good indicator of future behavior, this checks out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvDLxrkVi4kf"
      },
      "source": [
        "## 4. Calculating partial dependence and ICE to validate and explain monotonic behavior\n",
        "\n",
        "Partial dependence plots are used to view the global, average prediction behavior of a variable under the monotonic model. Partial dependence plots show the average prediction of the monotonic model as a function of specific values of an input variable of interest, indicating how the monotonic GBM predictions change based on the values of the input variable of interest, while taking nonlinearity into consideration and averaging out the effects of all other\n",
        "input variables. Partial dependence plots enable increased transparency into the monotonic GBM's mechanisms and enable validation and debugging of the monotonic GBM by comparing a variable's average predictions across its domain to known standards and reasonable expectations. Partial dependence plots are described in greater detail in *The Elements of Statistical Learning*, section 10.13: https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf.\n",
        "\n",
        "Individual conditional expectation (ICE) plots, a newer and less well-known adaptation of partial dependence plots, can be used to create more localized explanations for a single observation of data using the same basic ideas as partial dependence plots. ICE is also a type of nonlinear sensitivity analysis in which the model predictions for a single observation are measured while a feature of interest is varied over its domain. ICE increases understanding and transparency by displaying the nonlinear behavior of the monotonic GBM. ICE also enhances trust, accountability, and fairness by enabling comparisons of nonlinear behavior to human domain knowledge and reasonable expectations. ICE, as a type of sensitivity analysis, can also engender trust when model behavior on simulated or extreme data points is acceptable. A detailed description of ICE is available in this arXiv preprint: https://arxiv.org/abs/1309.6392.\n",
        "\n",
        "Because partial dependence and ICE are measured on the same scale, they can be displayed in the same line plot to compare the global, average prediction behavior for the entire model and the local prediction behavior for certain rows of data. Overlaying the two types of curves enables analysis of both global and local behavior simultaneously and provides an indication of the trustworthiness of the average behavior represented by partial dependence. (Partial dependence can be misleading in the presence of strong interactions or correlation. ICE curves diverging from the partial dependence curve can be indicative of such problems.) Histograms are also presented with the partial dependence and ICE curves, to enable a rough measure of epistemic uncertainty for model predictions: predictions based on small amounts of training data are likely less dependable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxFgkmzni4kg"
      },
      "source": [
        "#### Function for calculating partial dependence\n",
        "Since partial dependence and ICE will be calculated for several important variables in the GBM model, it's convenient to have a function doing so. It's probably best to analyze partial dependence and ICE for all variables in a model, but only the top three most important input variables will be investigated here. It's also a good idea to analyze partial dependence and ICE on the test data, or other holdout datasets, to see how the model will perform on new data. \n",
        "This simple function is designed to return partial dependence when it is called for an entire dataset and ICE when it is called for a single row. The `bins` argument will be used later to calculate ICE values at the same places in an input variable domain that partial dependence is calculated directly below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgLdZICvi4kg"
      },
      "source": [
        "def par_dep(xs, frame, model, resolution=20, bins=None):\n",
        "    \n",
        "    \"\"\" Creates Pandas DataFrame containing partial dependence for a \n",
        "        single variable.\n",
        "    \n",
        "    Args:\n",
        "        xs: Variable for which to calculate partial dependence.\n",
        "        frame: Pandas DataFrame for which to calculate partial dependence.\n",
        "        model: XGBoost model for which to calculate partial dependence.\n",
        "        resolution: The number of points across the domain of xs for which \n",
        "                    to calculate partial dependence, default 20.\n",
        "        bins: List of values at which to set xs, default 20 equally-spaced \n",
        "              points between column minimum and maximum.\n",
        "    \n",
        "    Returns:\n",
        "        Pandas DataFrame containing partial dependence values.\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    # turn off pesky Pandas copy warning\n",
        "    pd.options.mode.chained_assignment = None\n",
        "    \n",
        "    # initialize empty Pandas DataFrame with correct column names\n",
        "    par_dep_frame = pd.DataFrame(columns=[xs, 'partial_dependence'])\n",
        "    \n",
        "    # cache original column values \n",
        "    col_cache = frame.loc[:, xs].copy(deep=True)\n",
        "  \n",
        "    # determine values at which to calculate partial dependence\n",
        "    if bins == None:\n",
        "        min_ = frame[xs].min()\n",
        "        max_ = frame[xs].max()\n",
        "        by = (max_ - min_)/resolution\n",
        "        bins = np.arange(min_, max_, by)\n",
        "        \n",
        "    # calculate partial dependence  \n",
        "    # by setting column of interest to constant \n",
        "    # and scoring the altered data and taking the mean of the predictions\n",
        "    for j in bins:\n",
        "        frame.loc[:, xs] = j\n",
        "        dframe = xgb.DMatrix(frame)\n",
        "        par_dep_i = pd.DataFrame(model.predict(dframe, ntree_limit=model.best_ntree_limit))\n",
        "        par_dep_j = par_dep_i.mean()[0]\n",
        "        par_dep_frame = par_dep_frame.append({xs:j,\n",
        "                                              'partial_dependence': par_dep_j}, \n",
        "                                              ignore_index=True)\n",
        "        \n",
        "    # return input frame to original cached state    \n",
        "    frame.loc[:, xs] = col_cache\n",
        "\n",
        "    return par_dep_frame\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqEQ2vxBi4kh"
      },
      "source": [
        "#### Calculate partial dependence for the most important input variables in the GBM\n",
        "The partial dependence for `LIMIT_BAL` can be seen to decrease as credit balance limits increase. This finding is aligned with expectations that the model predictions will be monotonically decreasing with increasing `LIMIT_BAL` and parsimonious with well-known business practices in credit lending. Partial dependence for other important values is displayed in plots further below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "prUazBoqi4ki",
        "outputId": "41c2f5d9-4a84-43f1-b247-33ddca0f5044"
      },
      "source": [
        "par_dep_PAY_0 = par_dep('PAY_0', test[X], xgb_model)         # calculate partial dependence for PAY_0\n",
        "par_dep_LIMIT_BAL = par_dep('LIMIT_BAL', test[X], xgb_model) # calculate partial dependence for LIMIT_BAL\n",
        "par_dep_BILL_AMT1 = par_dep('BILL_AMT1', test[X], xgb_model) # calculate partial dependence for BILL_AMT1\n",
        "\n",
        "# display partial dependence for LIMIT_BAL\n",
        "par_dep_LIMIT_BAL"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LIMIT_BAL</th>\n",
              "      <th>partial_dependence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000.0</td>\n",
              "      <td>0.188235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>59500.0</td>\n",
              "      <td>0.202346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>109000.0</td>\n",
              "      <td>0.207201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>158500.0</td>\n",
              "      <td>0.212002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>208000.0</td>\n",
              "      <td>0.217106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>257500.0</td>\n",
              "      <td>0.220195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>307000.0</td>\n",
              "      <td>0.225494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>356500.0</td>\n",
              "      <td>0.230202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>406000.0</td>\n",
              "      <td>0.230931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>455500.0</td>\n",
              "      <td>0.236069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>505000.0</td>\n",
              "      <td>0.239475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>554500.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>604000.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>653500.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>703000.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>752500.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>802000.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>851500.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>901000.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>950500.0</td>\n",
              "      <td>0.239502</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    LIMIT_BAL  partial_dependence\n",
              "0     10000.0            0.188235\n",
              "1     59500.0            0.202346\n",
              "2    109000.0            0.207201\n",
              "3    158500.0            0.212002\n",
              "4    208000.0            0.217106\n",
              "5    257500.0            0.220195\n",
              "6    307000.0            0.225494\n",
              "7    356500.0            0.230202\n",
              "8    406000.0            0.230931\n",
              "9    455500.0            0.236069\n",
              "10   505000.0            0.239475\n",
              "11   554500.0            0.239502\n",
              "12   604000.0            0.239502\n",
              "13   653500.0            0.239502\n",
              "14   703000.0            0.239502\n",
              "15   752500.0            0.239502\n",
              "16   802000.0            0.239502\n",
              "17   851500.0            0.239502\n",
              "18   901000.0            0.239502\n",
              "19   950500.0            0.239502"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxFNJr3zi4kj"
      },
      "source": [
        "#### Helper function for finding percentiles of predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVkhh1F8i4kk"
      },
      "source": [
        "ICE can be calculated for any row in the training or test data, but without intimate knowledge of a data source it can be difficult to know where to apply ICE. Calculating and analyzing ICE curves for every row of training and test data set can be overwhelming, even for the example credit card default dataset. One place to start with ICE is to calculate ICE curves at every decile of predicted probabilities in a dataset, giving an indication of local prediction behavior across the dataset. The function below finds and returns the row indices for the maximum, minimum, and deciles of one column in terms of another -- in this case, the model predictions (`p_DEFAULT_NEXT_MONTH`) and the row identifier (`ID`), respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV-41wpWi4kk"
      },
      "source": [
        "def get_percentile_dict(yhat, id_, frame):\n",
        "\n",
        "    \"\"\" Returns the percentiles of a column, yhat, as the indices based on \n",
        "        another column id_.\n",
        "    \n",
        "    Args:\n",
        "        yhat: Column in which to find percentiles.\n",
        "        id_: Id column that stores indices for percentiles of yhat.\n",
        "        frame: Pandas DataFrame containing yhat and id_. \n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of percentile values and index column values.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # create a copy of frame and sort it by yhat\n",
        "    sort_df = frame.copy(deep=True)\n",
        "    sort_df.sort_values(yhat, inplace=True)\n",
        "    sort_df.reset_index(inplace=True)\n",
        "    \n",
        "    # find top and bottom percentiles\n",
        "    percentiles_dict = {}\n",
        "    percentiles_dict[0] = sort_df.loc[0, id_]\n",
        "    percentiles_dict[99] = sort_df.loc[sort_df.shape[0]-1, id_]\n",
        "\n",
        "    # find 10th-90th percentiles\n",
        "    inc = sort_df.shape[0]//10\n",
        "    for i in range(1, 10):\n",
        "        percentiles_dict[i * 10] = sort_df.loc[i * inc,  id_]\n",
        "\n",
        "    return percentiles_dict\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpqFX1avi4kl"
      },
      "source": [
        "#### Find some percentiles of yhat in the test data\n",
        "The values for `ID` that correspond to the maximum, minimum, and deciles of `p_DEFAULT_NEXT_MONTH` are displayed below. ICE will be calculated for the rows of the test dataset associated with these `ID` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqL_aD1Zi4km"
      },
      "source": [
        "# merge GBM predictions onto test data\n",
        "yhat_test = pd.concat([test.reset_index(drop=True), pd.DataFrame(xgb_model.predict(dtest, \n",
        "                                                                                   ntree_limit=xgb_model.best_ntree_limit))],\n",
        "                      axis=1)\n",
        "yhat_test = yhat_test.rename(columns={0:'p_DEFAULT_NEXT_MONTH'})\n",
        "\n",
        "# find percentiles of predictions\n",
        "percentile_dict = get_percentile_dict('p_DEFAULT_NEXT_MONTH', 'ID', yhat_test)\n",
        "\n",
        "# display percentiles dictionary\n",
        "# ID values for rows\n",
        "# from lowest prediction \n",
        "# to highest prediction\n",
        "percentile_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-meQ656i4kn"
      },
      "source": [
        "#### Calculate ICE curve values\n",
        "ICE values represent a model's prediction for a row of data while an input variable of interest is varied across its domain. The values of the input variable are chosen to match the values at which partial dependence was calculated earlier, and ICE is calculated for the top three most important variables and for rows at each percentile of the test dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3quYP24Ai4ko"
      },
      "source": [
        "# retreive bins from original partial dependence calculation\n",
        "\n",
        "bins_PAY_0 = list(par_dep_PAY_0['PAY_0'])\n",
        "bins_LIMIT_BAL = list(par_dep_LIMIT_BAL['LIMIT_BAL'])\n",
        "bins_BILL_AMT1 = list(par_dep_BILL_AMT1['BILL_AMT1'])\n",
        "\n",
        "# for each percentile in percentile_dict\n",
        "# create a new column in the par_dep frame \n",
        "# representing the ICE curve for that percentile\n",
        "# and the variables of interest\n",
        "for i in sorted(percentile_dict.keys()):\n",
        "    \n",
        "    col_name = 'Percentile_' + str(i)\n",
        "    \n",
        "    # ICE curves for PAY_0 across percentiles at bins_PAY_0 intervals\n",
        "    par_dep_PAY_0[col_name] = par_dep('PAY_0', \n",
        "                                    test[test['ID'] == int(percentile_dict[i])][X],  \n",
        "                                    xgb_model, \n",
        "                                    bins=bins_PAY_0)['partial_dependence']\n",
        "    \n",
        "    # ICE curves for LIMIT_BAL across percentiles at bins_LIMIT_BAL intervals\n",
        "    par_dep_LIMIT_BAL[col_name] = par_dep('LIMIT_BAL', \n",
        "                                          test[test['ID'] == int(percentile_dict[i])][X], \n",
        "                                          xgb_model, \n",
        "                                          bins=bins_LIMIT_BAL)['partial_dependence']\n",
        "    \n",
        "\n",
        "\n",
        "    # ICE curves for BILL_AMT1 across percentiles at bins_BILL_AMT1 intervals\n",
        "    par_dep_BILL_AMT1[col_name] = par_dep('BILL_AMT1', \n",
        "                                          test[test['ID'] == int(percentile_dict[i])][X],  \n",
        "                                          xgb_model, \n",
        "                                          bins=bins_BILL_AMT1)['partial_dependence']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puvxfzDli4kp"
      },
      "source": [
        "#### Display partial dependence and ICE for `LIMIT_BAL`\n",
        "Partial dependence and ICE values for rows at the minimum, maximum and deciles (0%, 10%, 20%, ..., 90%, 99%) of predictions for `DEFAULT_NEXT_MONTH` and at the values of `LIMIT_BAL` used for partial dependence are shown here. Each column of ICE values will be a curve in the plots below. ICE values represent a prediction for a row of test data, at a percentile of interest noted in the column name above, and setting `LIMIT_BAL` to the value of `LIMIT_BAL` at right. Notice that monotonic decreasing prediction behavior for `LIMIT_BAL` holds at each displayed percentile of predicted `DEFAULT_NEXT_MONTH`, helping to validate that the trained GBM predictions are monotonic for `LIMIT_BAL`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdTzHIari4kp"
      },
      "source": [
        "par_dep_LIMIT_BAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFa7YAXii4kq"
      },
      "source": [
        "## 5. Plotting partial dependence and ICE to validate and explain monotonic behavior\n",
        "\n",
        "Overlaying partial dependence onto ICE in a plot is a convenient way to validate and understand both global and local monotonic behavior. Plots of partial dependence curves overlayed onto ICE curves for several percentiles of predictions for `DEFAULT_NEXT_MONTH` are used to validate monotonic behavior, describe the GBM model mechanisms, and to compare the most extreme GBM behavior with the average GBM behavior in the test data. Partial dependence and ICE plots are displayed for the three most important variables in the GBM: `PAY_0`, `LIMIT_BAL`, and `BILL_AMT1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd-YVrPni4kr"
      },
      "source": [
        "#### Function to plot partial dependence and ICE\n",
        "\n",
        "def plot_par_dep_ICE(xs, par_dep_frame):\n",
        "\n",
        "    \n",
        "    \"\"\" Plots ICE overlayed onto partial dependence for a single variable.\n",
        "    \n",
        "    Args: \n",
        "        xs: Name of variable for which to plot ICE and partial dependence.\n",
        "        par_dep_frame: Name of Pandas DataFrame containing ICE and partial\n",
        "                       dependence values.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize figure and axis\n",
        "    fig, ax = plt.subplots()\n",
        "    \n",
        "    # plot ICE curves\n",
        "    par_dep_frame.drop('partial_dependence', axis=1).plot(x=xs, \n",
        "                                                          colormap='gnuplot',\n",
        "                                                          ax=ax)\n",
        "\n",
        "    # overlay partial dependence, annotate plot\n",
        "    par_dep_frame.plot(title='Partial Dependence and ICE for ' + str(xs),\n",
        "                       x=xs, \n",
        "                       y='partial_dependence',\n",
        "                       style='r-', \n",
        "                       linewidth=3, \n",
        "                       ax=ax)\n",
        "\n",
        "    # add legend\n",
        "    _ = plt.legend(bbox_to_anchor=(1.05, 0),\n",
        "                   loc=3, \n",
        "                   borderaxespad=0.)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LViEgyspi4ks"
      },
      "source": [
        "#### Partial dependence and ICE plot for `LIMIT_BAL`\n",
        "The monotonic prediction behavior displayed in the partial dependence, and ICE tables for `LIMIT_BAL` is also visible in this plot. Monotonic decreasing behavior is evident at every percentile of predictions for `DEFAULT_NEXT_MONTH`. Most percentiles of predictions show that sharper decreases in probability of default occur when `LIMIT_BAL` increases just slightly from its lowest values in the test set. However, for the custumers that are most likely to default according to the GBM model, no increase in `LIMIT_BAL` has a strong impact on probabilitiy of default. As mentioned previously, the displayed relationship between credit balance limits and probablility of default is not uncommon in credit lending. As can be seen from the displayed histogram, above ~$NT 500,000 prediction behavior may have been learned from extremely small samples of data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmXyHBJJi4kt"
      },
      "source": [
        "plot_par_dep_ICE('LIMIT_BAL', par_dep_LIMIT_BAL) # plot partial dependence and ICE for LIMIT_BAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUO4dmHGi4ku"
      },
      "source": [
        "_ = train['LIMIT_BAL'].plot(kind='hist', bins=20, title='Histogram: LIMIT_BAL')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuvt7tLIi4kv"
      },
      "source": [
        "#### Partial dependence and ICE plot for `PAY_0`\n",
        "Monotonic increasing prediction behavior for `PAY_0` is displayed for all percentiles of model predictions. Predition behavior is different at different deciles, but not abnormal or vastly different from the average prediction behavior represented by the red partial dependence curve. The largest jump in predicted probability appears to occur at `PAY_0 = 2`, or when a customer becomes two months late on their most recent payment. Above `PAY_0 = 3` there are few examples from which the model could learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgYQfRFti4kv"
      },
      "source": [
        "plot_par_dep_ICE('PAY_0', par_dep_PAY_0) # plot partial dependence and ICE for PAY_0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsdvOJw4i4kw"
      },
      "source": [
        "_ = train['PAY_0'].plot(kind='hist', bins=20, title='Histogram: PAY_0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCu0aMbbi4kx"
      },
      "source": [
        "#### Partial dependence and ICE plot for `BILL_AMT1`\n",
        "Monotonic decreasing prediction behavior for `BILL_AMT1` is also displayed for all percentiles. This mild decrease in probability of default as most recent bill amount increases could be related to wealthier, big-spending customers taking on more debt but also being able to pay it off reliably. Also, customers with negative bills are more likely to default, potentially indicating charge-offs are being recorded as negative bills. In a mission-critical situation, this issue would require more debugging. Also predictions below \\$ NT 0 and above \\$ NT 400,000 are based on very little training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8DYWIzLIi4kx"
      },
      "source": [
        "plot_par_dep_ICE('BILL_AMT1', par_dep_BILL_AMT1) # plot partial dependence and ICE for BILL_AMT1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxDCu8W0i4ky"
      },
      "source": [
        "_ = train['BILL_AMT1'].plot(kind='hist', bins=20, title='Histogram: BILL_AMT1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfOE0VMNi4kz"
      },
      "source": [
        "## 6. Generate reason codes using the Shapley method \n",
        "Now that the monotonic behavior of the GBM has been verified and compared against domain knowledge and reasonable expectations, a method called Shapley explanations will be used to calculate the local variable importance for any one prediction: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions. Shapley explanations are the only possible consistent local variable importance values. (Here consistency means that if a variable is more important than another variable in a given prediction, the more important variable's Shapley value will not be smaller in magnitude than the less important variable's Shapley value.) Very crucially Shapley values also *always* sum to the actual prediction of the XGBoost model. When used in a model-specific context for decision tree models, Shapley values are likely the most accurate known local variable importance method available today. In this notebook, XGBoost itself is used to create Shapley values with the `pred_contribs` parameter to `predict()`, but the `shap` package is also available for other types of models: https://github.com/slundberg/shap. \n",
        "\n",
        "The numeric Shapley values in each column are an estimate of how much each variable contributed to each prediction. Shapley contributions can indicate how a variable and its values were weighted in any given decision by the model. These values are crucially important for machine learning interpretability and are related to \"local feature importance\", \"reason codes\", or \"turn-down codes.\" The latter phrases are borrowed from credit scoring. Credit lenders in the U.S. must provide reasons for automatically rejecting a credit application. Reason codes can be easily extracted from Shapley local variable contribution values by ranking the variables that played the largest role in any given decision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQPIGV5Oi4kz"
      },
      "source": [
        "To find the index corresponding to a particular row of interest later, the index of the `test` DataFrame is reset to begin at 0 and increase sequentially. Without resetting the index, the `test` DataFrame row indices still correspond to the original raw data from which the test set was sampled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzLgKVzZi4k0"
      },
      "source": [
        "test.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-45zN_Opi4k0"
      },
      "source": [
        "#### Select most risky customer in test data\n",
        "One person who might be of immediate interest is the most likely to default customer in the test data. This customer's row will be selected and local variable importance for the corresponding prediction will be analyzed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsENXqZ4i4k1"
      },
      "source": [
        "decile = 99\n",
        "row = test[test['ID'] == percentile_dict[decile]]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvAd0VFCi4k1"
      },
      "source": [
        "#### Create a Pandas DataFrame of Shapley values for riskiest customer\n",
        "The most interesting Shapley values are probably those that push this customer's probability of default higher, i.e. the highest positive Shapley values. Those values are plotted below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxXvfJVli4k1"
      },
      "source": [
        "# reset test data index to find riskiest customer in shap_values \n",
        "# sort to find largest positive contributions\n",
        "s_df = pd.DataFrame(shap_values[row.index[0], :][:-1].reshape(23, 1), columns=['Reason Codes'], index=X)\n",
        "s_df.sort_values(by='Reason Codes', inplace=True, ascending=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMqyGknki4k2"
      },
      "source": [
        "s_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GVOrDxi4k2"
      },
      "source": [
        "#### Plot top local contributions as reason codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbuo89i4i4k3"
      },
      "source": [
        "_ = s_df[:5].plot(kind='bar', \n",
        "                  title='Top Five Reason Codes for a Risky Customer\\n', \n",
        "                  legend=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkASixIri4k3"
      },
      "source": [
        "For the customer in the test dataset that the GBM predicts as most likely to default, the most important input variables in the prediction are, in descending order, `PAY_0`, `PAY_5`, `PAY_6`, `PAY_2`, and `LIMIT_BAL`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRSt6h4ki4k4"
      },
      "source": [
        "#### Display customer in question "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVom6-1Gi4k4"
      },
      "source": [
        "The local contributions for this customer appear reasonable, especially when considering her payment information. Her most recent payment was 3 months late and her payment for 6 months and 5 months previous were 7 months late. Also her credit limit was extremely low, so it's logical that these factors would weigh heavily into the model's prediction for default for this customer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuiqUGUVi4k5"
      },
      "source": [
        "row # helps understand reason codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OwnNyLi4k5"
      },
      "source": [
        "To generate reason codes for the model's decision, the locally important variable and its value are used together. If this customer was denied future credit based on this model and data, the top five Shapley-based reason codes for the automated decision would be:\n",
        "\n",
        "1. Most recent payment is 3 months delayed.\n",
        "2. 5th most recent payment is 7 months delayed.\n",
        "3. 6th most recent payment is 7 months delayed.\n",
        "4. 2nd most recent payment is 2 months delayed.\n",
        "5. Credit limit is too low: 10,000 $NT.\n",
        "\n",
        "(Of course, credit limits are set by the lender and are used to price-in risk to credit decisions, so using credit limits as reason codes or even in a probability of default model is likely questionable. However, in this small, example data set all input columns were used to generate a better model fit. For a slightly more careful treatment of gradient boosting in the context of credit scoring, please see: https://github.com/jphall663/interpretable_machine_learning_with_python/blob/master/dia.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4doiJCF3i4k6"
      },
      "source": [
        "#### Summary\n",
        "\n",
        "In this notebook, a highly transparent, nonlinear, monotonic GBM classifier was trained to predict credit card defaults and the monotonic behavior of the classifier was analyzed and validated. To do so, Pearson correlation between each input and the target was used to determine the direction for monotonicity constraints for each input variable in the XGBoost classifier. GBM variable importance, partial dependence, and ICE were calculated, plotted, and compared to one another, domain knowledge, and reasonable expectations. Shapley values were then used to explain the model predictions for the single most risky customer in the test set. These techniques should generalize well for many types of business and research problems, enabling you to train a monotonic GBM model and analyze, validate, and explain it to your colleagues, bosses, and potentially, external regulators. "
      ]
    }
  ]
}